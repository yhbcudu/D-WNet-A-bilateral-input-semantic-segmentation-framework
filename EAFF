class DenseBlock(nn.Module):
    def __init__(self, in_channels, growth_rate, num_layers):
        super(DenseBlock, self).__init__()
        self.layers = nn.ModuleList()
        for _ in range(num_layers):
            self.layers.append(nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1))
            in_channels += growth_rate

    def forward(self, x):
        for layer in self.layers:
            out = layer(x)
            x = torch.cat([x, out], dim=1)
        return x


class MultiHeadAttention(nn.Module):
    def __init__(self, channels, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.channels = channels
        self.head_dim = channels // num_heads

        self.query = nn.Conv2d(channels, channels, kernel_size=1)
        self.key = nn.Conv2d(channels, channels, kernel_size=1)
        self.value = nn.Conv2d(channels, channels, kernel_size=1)

        self.fc_out = nn.Conv2d(channels, channels, kernel_size=1)

    def forward(self, x):
        B, C, H, W = x.size()
        Q = self.query(x).view(B, self.num_heads, self.head_dim, H * W)
        K = self.key(x).view(B, self.num_heads, self.head_dim, H * W)
        V = self.value(x).view(B, self.num_heads, self.head_dim, H * W)

        energy = torch.einsum('bqhd,bkhd->bhqk', [Q, K])
        attention = torch.softmax(energy, dim=-1)

        out = torch.einsum('bhqk,bvhd->bhqd', [attention, V]).contiguous().view(B, C, H, W)
        out = self.fc_out(out)
        return out


class AFF(nn.Module):
    def __init__(self, channels, num_heads=8):
        super(AFF, self).__init__()

        self.dense_block = DenseBlock(1024, 256, num_layers=3)

        self.local_att = nn.Sequential(
            nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)
        )
        self.global_att = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)
        )
        self.sigmoid = nn.Sigmoid()

        self.multi_head_att = MultiHeadAttention(channels, num_heads)

        self.channel_match = nn.Conv2d(1792, 1024, kernel_size=1)

    def forward(self, x1, x2):
        print(f"x2 shape before unbind: {x2.shape}")

        x2_split = torch.unbind(x2, dim=1)


        x2_processed = [self.dense_block(x) for x in x2_split]


        x2_processed = torch.stack(x2_processed, dim=1)

        print(f"x2_processed shape after DenseBlock: {x2_processed.shape}")

        x2_processed = x2_processed.view(-1, 1792, 2, 2)

        print(f"x2_processed shape after view: {x2_processed.shape}")

        x2_processed = self.channel_match(x2_processed)

        print(f"x2_processed shape after channel matching: {x2_processed.shape}")
        xl = x1 + x2_processed
        xl = self.local_att(xl)
        xl = self.sigmoid(xl)
        xg = x1 + x2_processed
        xg = self.global_att(xg)
        xg = self.sigmoid(xg)


        xo = x1 * xl + x2_processed * (1 - xl) + x1 * xg + x2_processed * (1 - xg)


        xo = self.multi_head_att(xo)

        return xo
