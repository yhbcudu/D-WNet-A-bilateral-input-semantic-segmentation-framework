class DenseBlock(nn.Module):
    def __init__(self, in_channels, growth_rate, num_layers):
        super(DenseBlock, self).__init__()
        self.layers = nn.ModuleList()
        for _ in range(num_layers):
            self.layers.append(nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1))
            in_channels += growth_rate

    def forward(self, x):
        for layer in self.layers:
            out = layer(x)
            x = torch.cat([x, out], dim=1)
        return x


class MultiHeadAttention(nn.Module):
    def __init__(self, channels, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.channels = channels
        self.head_dim = channels // num_heads

        self.query = nn.Conv2d(channels, channels, kernel_size=1)
        self.key = nn.Conv2d(channels, channels, kernel_size=1)
        self.value = nn.Conv2d(channels, channels, kernel_size=1)

        self.fc_out = nn.Conv2d(channels, channels, kernel_size=1)

    def forward(self, x):
        B, C, H, W = x.size()
        Q = self.query(x).view(B, self.num_heads, self.head_dim, H * W)
        K = self.key(x).view(B, self.num_heads, self.head_dim, H * W)
        V = self.value(x).view(B, self.num_heads, self.head_dim, H * W)

        energy = torch.einsum('bqhd,bkhd->bhqk', [Q, K])
        attention = torch.softmax(energy, dim=-1)

        out = torch.einsum('bhqk,bvhd->bhqd', [attention, V]).contiguous().view(B, C, H, W)
        out = self.fc_out(out)
        return out


class AFF(nn.Module):
    def __init__(self, channels, num_heads=8):
        super(AFF, self).__init__()

        self.local_att = nn.Sequential(
            nn.Conv2d(channels + 12 * channels, channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)
        )

        self.global_att = nn.Sequential(
            nn.AdaptiveAvgPool2d((2, 2)),
            nn.Conv2d(channels + 12 * channels, channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)
        )

        self.upsample = nn.Upsample(size=(64, 64), mode='bilinear', align_corners=False)

        self.sigmoid = nn.Sigmoid()
        self.multi_head_att = MultiHeadAttention(channels, num_heads)

    def forward(self, x1, x2):
        print(f"\n=== AFF (Attention Feature Fusion) ===")
        print(f"x1 (optical) shape: {x1.shape}")
        print(f"x2 (SAR) shape: {x2.shape}")

        batch_size, seq_len, channels, h, w = x2.size()
        x2_concat = x2.view(batch_size, seq_len * channels, h, w)
        print(f"x2 after concatenation: {x2_concat.shape}")

        combined = torch.cat([x1, x2_concat], dim=1)
        print(f"Combined features shape: {combined.shape}")

        xl = self.local_att(combined)
        print(f"xl after local attention: {xl.shape}")
        xl = self.sigmoid(xl)

        xg = self.global_att(combined)
        print(f"xg after global attention: {xg.shape}")
        xg = self.sigmoid(xg)

        xg = self.upsample(xg)
        print(f"xg after upsampling: {xg.shape}")

        xo = x1 * xl + x1 * xg
        print(f"xo after fusion: {xo.shape}")

        xo = self.multi_head_att(xo)
        print(f"AFF output: {xo.shape}")
        return xo
