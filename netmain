import torch
from torch import nn
from torch.nn import functional as F


class DepthwiseSeparableConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super(DepthwiseSeparableConv, self).__init__()
        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride,
                                   padding=padding, groups=in_channels)

        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)

    def forward(self, x):
        x = self.depthwise(x)
        x = self.pointwise(x)
        return x


class UNetEncoderBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(UNetEncoderBlock, self).__init__()
        self.conv1 = DepthwiseSeparableConv(in_channels, out_channels, kernel_size=3, padding=1)
        self.conv2 = DepthwiseSeparableConv(out_channels, out_channels, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = self.pool(x)
        return x


class OpticalImageProcessingNetwork(nn.Module):
    def __init__(self, in_channels=4, out_channels=1024):
        super(OpticalImageProcessingNetwork, self).__init__()

        self.encoder1 = UNetEncoderBlock(in_channels, 64)
        self.encoder2 = UNetEncoderBlock(64, 128)
        self.encoder3 = UNetEncoderBlock(128, 256)
        self.encoder4 = UNetEncoderBlock(256, 512)
        self.encoder5 = UNetEncoderBlock(512, 1024)

    def forward(self, x):
        x = self.encoder1(x)
        x = self.encoder2(x)
        x = self.encoder3(x)
        x = self.encoder4(x)
        x = self.encoder5(x)
        return x


class ConvLSTMCell(nn.Module):
    def __init__(self, input_size, hidden_size, kernel_size=3):
        super(ConvLSTMCell, self).__init__()
        self.hidden_size = hidden_size
        self.kernel_size = kernel_size
        self.conv = nn.Conv2d(input_size + hidden_size, 4 * hidden_size, kernel_size=kernel_size, padding=1)

    def forward(self, x, h_prev, c_prev):
        combined = torch.cat([x, h_prev], dim=1)
        conv_output = self.conv(combined)


        i, f, o, g = torch.split(conv_output, self.hidden_size, dim=1)

        i = torch.sigmoid(i)
        f = torch.sigmoid(f)
        o = torch.sigmoid(o)
        g = torch.tanh(g)

        c = f * c_prev + i * g
        h = o * torch.tanh(c)
        return h, c


class ConvLSTM(nn.Module):
    def __init__(self, input_channels, hidden_channels, kernel_size=3):
        super(ConvLSTM, self).__init__()
        self.hidden_channels = hidden_channels
        self.kernel_size = kernel_size
        self.cell = ConvLSTMCell(input_channels, hidden_channels, kernel_size)

    def forward(self, x):
        batch_size, seq_len, c, h, w = x.size()
        h_prev = torch.zeros(batch_size, self.hidden_channels, h, w).to(x.device)
        c_prev = torch.zeros(batch_size, self.hidden_channels, h, w).to(x.device)

        output_seq = []
        for t in range(seq_len):
            h_prev, c_prev = self.cell(x[:, t, :, :, :], h_prev, c_prev)
            output_seq.append(h_prev)

        output_seq = torch.stack(output_seq, dim=1)
        return output_seq



class ConvLSTMEncoderBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3):
        super(ConvLSTMEncoderBlock, self).__init__()
        self.conv_lstm = ConvLSTM(in_channels, out_channels, kernel_size)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

    def forward(self, x):

        batch_size, seq_len, c, h, w = x.size()


        x = self.conv_lstm(x)

        pooled_output = []
        for t in range(seq_len):
            pooled_output.append(self.pool(x[:, t, :, :, :]))

        pooled_output = torch.stack(pooled_output, dim=1)

        return pooled_output



class SARImageProcessingNetwork(nn.Module):
    def __init__(self, in_channels=1, seq_len=12, out_channels=1024):
        super(SARImageProcessingNetwork, self).__init__()

        self.encoder1 = ConvLSTMEncoderBlock(in_channels, 64)
        self.encoder2 = ConvLSTMEncoderBlock(64, 128)
        self.encoder3 = ConvLSTMEncoderBlock(128, 256)
        self.encoder4 = ConvLSTMEncoderBlock(256, 512)
        self.encoder5 = ConvLSTMEncoderBlock(512, out_channels)

    def forward(self, x):
        x = self.encoder1(x)
        x = self.encoder2(x)
        x = self.encoder3(x)
        x = self.encoder4(x)
        x = self.encoder5(x)

        return x


class DenseBlock(nn.Module):
    def __init__(self, in_channels, growth_rate, num_layers):
        super(DenseBlock, self).__init__()
        self.layers = nn.ModuleList()
        for _ in range(num_layers):
            self.layers.append(nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1))
            in_channels += growth_rate

    def forward(self, x):
        for layer in self.layers:
            out = layer(x)
            x = torch.cat([x, out], dim=1)
        return x


class MultiHeadAttention(nn.Module):
    def __init__(self, channels, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.channels = channels
        self.head_dim = channels // num_heads

        self.query = nn.Conv2d(channels, channels, kernel_size=1)
        self.key = nn.Conv2d(channels, channels, kernel_size=1)
        self.value = nn.Conv2d(channels, channels, kernel_size=1)

        self.fc_out = nn.Conv2d(channels, channels, kernel_size=1)

    def forward(self, x):
        B, C, H, W = x.size()
        Q = self.query(x).view(B, self.num_heads, self.head_dim, H * W)
        K = self.key(x).view(B, self.num_heads, self.head_dim, H * W)
        V = self.value(x).view(B, self.num_heads, self.head_dim, H * W)

        energy = torch.einsum('bqhd,bkhd->bhqk', [Q, K])
        attention = torch.softmax(energy, dim=-1)

        out = torch.einsum('bhqk,bvhd->bhqd', [attention, V]).contiguous().view(B, C, H, W)
        out = self.fc_out(out)
        return out


class AFF(nn.Module):
    def __init__(self, channels, num_heads=8):
        super(AFF, self).__init__()

        self.dense_block = DenseBlock(1024, 256, num_layers=3)

        self.local_att = nn.Sequential(
            nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)
        )
        self.global_att = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)
        )
        self.sigmoid = nn.Sigmoid()

        self.multi_head_att = MultiHeadAttention(channels, num_heads)


        self.channel_match = nn.Conv2d(1792, 1024, kernel_size=1)

    def forward(self, x1, x2):
        print(f"x2 shape before unbind: {x2.shape}")

        x2_split = torch.unbind(x2, dim=1)


        x2_processed = [self.dense_block(x) for x in x2_split]


        x2_processed = torch.stack(x2_processed, dim=1)

        print(f"x2_processed shape after DenseBlock: {x2_processed.shape}")

        x2_processed = x2_processed.view(-1, 1792, 2, 2)

        print(f"x2_processed shape after view: {x2_processed.shape}")

        x2_processed = self.channel_match(x2_processed)

        print(f"x2_processed shape after channel matching: {x2_processed.shape}")
        xl = x1 + x2_processed
        xl = self.local_att(xl)
        xl = self.sigmoid(xl)
        xg = x1 + x2_processed
        xg = self.global_att(xg)
        xg = self.sigmoid(xg)


        xo = x1 * xl + x2_processed * (1 - xl) + x1 * xg + x2_processed * (1 - xg)


        xo = self.multi_head_att(xo)

        return xo


class UNetDecoderBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(UNetDecoderBlock, self).__init__()
        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)
        self.conv1 = DepthwiseSeparableConv(in_channels, out_channels, kernel_size=3, padding=1)
        self.conv2 = DepthwiseSeparableConv(out_channels, out_channels, kernel_size=3, padding=1)

    def forward(self, x, skip_connection):
        x = self.upconv(x)
        x = torch.cat([x, skip_connection], dim=1)
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        return x


class OpticalImageDecoderNetwork(nn.Module):
    def __init__(self, in_channels=1024, out_channels=1):  #
        super(OpticalImageDecoderNetwork, self).__init__()

        self.decoder1 = UNetDecoderBlock(in_channels, 512)
        self.decoder2 = UNetDecoderBlock(512 + 512, 256)  #
        self.decoder3 = UNetDecoderBlock(256 + 256, 128)
        self.decoder4 = UNetDecoderBlock(128 + 128, 64)
        self.decoder5 = UNetDecoderBlock(64 + 64, 32)

        self.final_conv = nn.Conv2d(32, out_channels, kernel_size=1)

    def forward(self, x, skips):

        x = self.decoder1(x, skips[4])
        x = self.decoder2(x, skips[3])
        x = self.decoder3(x, skips[2])
        x = self.decoder4(x, skips[1])
        x = self.decoder5(x, skips[0])


        output = self.final_conv(x)
        return output


class FullOpticalImageClassificationModel(nn.Module):
    def __init__(self, in_channels=4, out_channels=1):
        super(FullOpticalImageClassificationModel, self).__init__()


        self.encoder = OpticalImageProcessingNetwork(in_channels=in_channels, out_channels=1024)


        self.aff = AFF(channels=1024)

        self.sar_network = SARImageProcessingNetwork(in_channels=1, seq_len=12, out_channels=1024)


        self.decoder = OpticalImageDecoderNetwork(in_channels=1024, out_channels=out_channels)

    def forward(self, optical_images, sar_images):

        optical_features = self.encoder(optical_images)


        sar_features = self.sar_network(sar_images)


        fused_features = self.aff(optical_features, sar_features)


        skips = [optical_features]
        classification_output = self.decoder(fused_features, skips)

        return classification_output

optical_images = torch.randn(1, 4, 64, 64)
sar_images = torch.randn(1, 12, 1, 64, 64)

model = FullOpticalImageClassificationModel(in_channels=4, out_channels=1)
output = model(optical_images, sar_images)
print(output.shape)
